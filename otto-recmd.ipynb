{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MbtowAcnw6wc"},"outputs":[],"source":["!pip install cuda-python"]},{"cell_type":"markdown","metadata":{"id":"v_v9rtdLEPq3"},"source":["#Preprocessing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oASkyWJTEMJf"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","\n","# calculate the number of events associated with each item\n","# returns a dictionary {aid: count_events}\n","def cal_eventsperitem(filepath):\n","    chunk_size = 10000\n","\n","    aid_counts = {}\n","\n","    chunks = pd.read_json(filepath, lines=True, chunksize=chunk_size)\n","    for i, chunk in enumerate(chunks):\n","        events = chunk['events']\n","        events_exploded = events.explode('events')\n","        aid_values = events_exploded.apply(lambda x: x['aid'])\n","\n","        aid_count = aid_values.value_counts()\n","        for key, value in aid_count.items():\n","            if key in aid_counts:\n","                aid_counts[key] += value\n","            else:\n","                aid_counts[key] = value\n","    return aid_counts\n","\n","\n","file_path = \"/kaggle/input/otto-recommender-system/train.jsonl\"\n","aidcounts = cal_eventsperitem(file_path)\n","print(len(aidcounts))\n","\n","# get a list of aids that should be filtered out\n","# save this list to to_remove.csv file\n","total_n_events = sum(aidcounts.values())\n","freqency_limit = 10\n","\n","words = [word for word, freq in aidcounts.items() if freq < freqency_limit]\n","freqs = [freq for word, freq in aidcounts.items() if freq < freqency_limit]\n","\n","aid_fre_low = {\"word\": words, \"freq\": freqs}\n","df = pd.DataFrame(aid_fre_low)\n","df.to_csv(\"to_remove.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBPZxWBNExSx"},"outputs":[],"source":["#remove all the items with low frequency\n","#ori_filepath: the path to the origianl train data file\n","#filtered_filepath: the path to the file to save the filtered train data\n","#aid_to_remove: a list of items to be removed from the original data\n","\n","def filterdata(ori_filepath, filtered_filepath, aid_to_remove):\n","    chunk_size = 10000\n","    sessioncount = 0\n","    chunks = pd.read_json(ori_filepath, lines=True, chunksize=chunk_size)\n","    for i, chunk in enumerate(chunks):\n","        # only remove events, not sessions\n","        # exp_chunk = chunk.explode('events')\n","        # exp_filter_chunk = exp_chunk[~exp_chunk['events'].apply(lambda x: x['aid'] in aid_to_remove.keys())]\n","        # result = exp_filter_chunk.groupby('session').agg({'events':lambda x: list(x)})\n","        # result = result.reset_index(drop=False)\n","        #remove all sessions with events in to_be_removed\n","        exp_filter_chunk = chunk[~chunk['events'].apply(lambda x: any(d['aid'] in aid_to_remove.keys() for d in x))]\n","        result = exp_filter_chunk\n","        #covert the data frame to JSONL format\n","\n","        sessioncount += result.shape[0]\n","        jsonl_str = result.to_json(orient='records',lines=True)\n","        with open(filtered_filepath, 'a') as f:\n","            f.write(jsonl_str + '\\n')\n","    return sessioncount\n","ori_filepath = \"/kaggle/input/otto-recommender-system/train.jsonl\"\n","filtered_filepath = \"/kaggle/temp/filtered_train.jsonl\"\n","\n","csv_pd = pd.read_csv(\"/kaggle/working/to_remove.csv\")\n","aid_to_remove = dict(zip(csv_pd['word'], csv_pd['freq']))\n","sess_count = filterdata(ori_filepath, filtered_filepath,aid_to_remove)\n","print(sess_count)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhKh_onmFFV6"},"outputs":[],"source":["import os\n","# get the encoding for the aid, the new value of aid from should be from 0 to # of aids..\n","# return two dictionaries {oldaid: newaid} {newaid: oldaid}\n","def encode_aid(filepath):\n","    aidnumencoding = {}\n","    numaidencoding = {}\n","    newid = 1\n","    chunk_size = 10000\n","    chunks = pd.read_json(filepath, lines=True, chunksize=chunk_size)\n","    for i, chunk in enumerate(chunks):\n","        aids = chunk['events'].apply(lambda x: [d['aid'] for d in x])\n","        for row in aids:\n","            for aid in row:\n","                if aid not in aidnumencoding.keys():\n","                    numaidencoding[newid] = aid\n","                    aidnumencoding[aid] = newid\n","                    newid += 1\n","    return aidnumencoding, numaidencoding\n","\n","\n","aidnumencoding, numaidencoding = encode_aid(\"/kaggle/input/cleaneddata/filtered_train.jsonl\")\n","# os.remove(\"/kaggle/working/filtered_train.jsonl\")\n","\n","aids = aidnumencoding.keys()\n","nums = aidnumencoding.values()\n","aidnum = {'aids': aids, 'nums': nums}\n","df = pd.DataFrame(aidnum)\n","df.to_csv('aid_num_encoding.csv')\n","\n","nums = numaidencoding.keys()\n","aids = numaidencoding.values()\n","numaid = {'nums': nums, 'aids': aids}\n","df = pd.DataFrame(numaid)\n","df.to_csv('num_aid_encoding.csv')\n","\n","\n","#get the encoding dictionary from the csv file\n","codingpath = '/kaggle/input/aid-num-encoding/aid_num_encoding.csv'\n","df = pd.read_csv(codingpath)\n","df = df[['aids','nums']]\n","aidnumencoding = dict(zip(df['aids'],df['nums']))\n","print(len(aidnumencoding))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGzYlPUNFNup"},"outputs":[],"source":["#split the train dataset into different files according to the length of the event sequence\n","#encode the item id into 1, 2, 3, ...\n","def encoding(filepath, aidnumencoding):\n","    type_encoding = {'clicks': 1, 'carts': 2, 'orders': 3}\n","    length_ranges = list(range(5, 45, 5))\n","    length_ranges_long = list(range(50, 550, 50))\n","    length_ranges += length_ranges_long\n","\n","    file_seq_count = {}\n","    for i, max_length in enumerate(length_ranges):\n","        if i == 0:\n","            min_length = 0\n","        else:\n","            min_length = length_ranges[i - 1]\n","        seq_count = 0\n","        event_seqs = []\n","        chunks = pd.read_json(filepath, lines=True, chunksize=10000)\n","        for chunk in chunks:\n","            for row in chunk.to_numpy():\n","                session_id, events = row\n","                if min_length < len(events) <= max_length:\n","                    event_items = [aidnumencoding[event['aid']] for event in events]\n","                    event_actions = [type_encoding[event['type']] for event in events]\n","                    n_paddings = max_length - len(events)\n","                    event_items = [0] * n_paddings + event_items\n","                    event_actions = [0] * n_paddings + event_actions\n","                    event_seqs.append(np.array([event_items, event_actions]))\n","                    seq_count += 1\n","        #             break\n","        event_seqs_np = np.array(event_seqs)\n","\n","        np.save(f\"./preprocessed/seq_{max_length}\", event_seqs_np)\n","\n","        file_seq_count[max_length] = seq_count\n","\n","    print(file_seq_count)\n","\n","\n","filepath = \"/kaggle/input/cleaneddata/filtered_train.jsonl\"\n","encoding(filepath, aidnumencoding)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLhos4RCFWg-"},"outputs":[],"source":["#randomly split each train file into train data and validation data by 8:2\n","def split_train_validate(file_paths):\n","    for file in file_paths:\n","        sessions = np.load(file)\n","        size = sessions.shape[0]\n","        print(f\"size of the file {size}\")\n","        # Randomly select N items\n","        selected_indices = np.random.choice(sessions.shape[0], size= int(size * 0.2), replace=False)\n","        selected_sessions = sessions[selected_indices]\n","        print(f\"size of selected sessions {selected_sessions.shape[0]}\")\n","        # Remove selected items from original array\n","        remaining_sessions = np.delete(sessions, selected_indices, axis=0)\n","        print(f\"size of remaining sessions {remaining_sessions.shape[0]}\")\n","        filename = os.path.splitext(os.path.basename(file))[0]\n","        np.save(f\"./preprocessed/train_{filename}\", remaining_sessions)\n","        np.save(f\"./preprocessed/validation_{filename}\", selected_sessions)\n","\n","# Example directory path\n","dir_path = \"/kaggle/input/cleaned-data/\"\n","\n","# Get file paths of all files in directory tree\n","file_paths = []\n","for root, dirs, files in os.walk(dir_path):\n","    for file in files:\n","        file_paths.append(os.path.join(root, file))\n","# os.makedirs(\"./preprocessed\")\n","split_train_validate(file_paths)"]},{"cell_type":"markdown","metadata":{"id":"vzEjMU78FkoT"},"source":["#Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4JRA6vjwr1H"},"outputs":[],"source":["import torch\n","from torch import Tensor\n","import numpy as np\n","\n","\n","def ndcg_at_ip(predictions: Tensor, positive_index: Tensor, k: Tensor):\n","    \"\"\"\n","    Computes the discounted cumulative gain (DCG) at a given value of k.\n","\n","    Args:\n","    - scores: a tensor of relevance scores (in descending order) of recommended items.\n","    - positive_index: a positive integer specifying the number of top items to be considered.\n","\n","    Returns:\n","    - ndcg: a value of the DCG score.\n","    \"\"\"\n","    dcg = 0.0\n","    for i, p in enumerate(predictions[:k]):\n","        if p == positive_index:\n","            dcg += 1.0 / float(np.log2(i + 2.0))\n","\n","    return dcg\n","\n","def ndcg_batch(scores: Tensor, positive_batch: Tensor):\n","    batch_dcg = 0\n","    for i in range(len(positive_batch)):\n","        top_n = torch.topk(scores[i, :], 100).indices.tolist()\n","        score = ndcg_at_ip(top_n, positive_batch[i], 20)\n","        batch_dcg += score\n","    return batch_dcg"]},{"cell_type":"markdown","metadata":{"id":"CTXRl21zF_HB"},"source":["#Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MRgVqu8wuS2"},"outputs":[],"source":["from torch.utils.data import IterableDataset\n","from torch.utils.data import DataLoader\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","#parameter\n","negative_sample_size = 1\n","num_item = 364846 + 2\n","class JsonlDataset(IterableDataset):\n","    def __init__(self, filepaths):\n","        self.filepaths = filepaths\n","        random.shuffle(self.filepaths)\n","        # self.files = [pd.read_json(filepath, lines=True) for filepath in self.filepaths]\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        for file in self.filepaths:\n","            json_df = pd.read_json(file, lines=True)\n","            json_df['events'] = json_df['events'].apply(lambda x: [[d['ts'], d['type'], d['aid']] for d in x])\n","            print(json_df.head())\n","            numdata = json_df.to_numpy()\n","            random.shuffle(numdata)\n","            yield numdata\n","\n","class NumpyDataset(IterableDataset):\n","    def __init__(self, filepaths, batch_size):\n","        self.filepaths = filepaths\n","        self.batch_size = batch_size\n","        random.shuffle(self.filepaths)\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        for file in self.filepaths:\n","            data = np.load(file)\n","            random.shuffle(data)\n","            batches = np.array_split(data, len(data) // self.batch_size)\n","            for batch in batches:\n","                yield {\"items\": batch, \"negatives\": np.random.randint(1, num_item,\n","                                                                      size=(len(batch),\n","                                                                            len(batch[0][0]),\n","                                                                            negative_sample_size))}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wDipVThkF8ix"},"source":["#Transformer Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sJWwkNBwf2o"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from torch import nn, Tensor\n","import torch\n","\n","\n","#the function to pad the input sequence\n","from torch.utils.data import IterableDataset\n","\n","def generate_encoder_input_mask(dim: int, batchsize: int, num_heads: int) -> Tensor:\n","    \"\"\"\n","    Args:\n","        dim: int, for src masking this must be encoder sequence length (i.e.\n","              the length of the input sequence to the model)\n","    Return:\n","        A Tensor of shape (batch_size, num_heads, dim, dim)\n","    \"\"\"\n","    mask = torch.triu(torch.ones(dim, dim) * float('-inf'), diagonal=1)\n","    # mask = mask.repeat(batchsize * num_heads, 1, 1)\n","    return mask\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZdKeazewwQm"},"outputs":[],"source":["import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","import json\n","import io\n","import math\n","import torch\n","from typing import Optional, Any, Union, Callable\n","import torch.nn as nn\n","from torch.nn.modules.normalization import LayerNorm\n","\n","from torch import Tensor\n","import torch.nn.functional as F\n","\n","\n","\n","# define the transformer model\n","class TimeSeriesTransformer(nn.Module):\n","    \"\"\"\n","        args:\n","\n","        shapes:\n","    \"\"\"\n","    def __init__(self,\n","                 num_item,\n","                 output_size,\n","                 hidden_size,\n","                 num_layers=4,\n","                 num_heads=4,\n","                 dim_feedforward_encoder=256,\n","                 dropout=0.2,\n","                 max_length=500,\n","                 layer_norm_eps: float = 1e-5,\n","                 device = None,\n","                 dtype = None\n","                 ):\n","        factory_kwargs = {'device': device, 'dtype': dtype}\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.nhead = num_heads\n","\n","        self.embedding_aid = nn.Embedding(num_embeddings=num_item + 1, embedding_dim=hidden_size, padding_idx=0, **factory_kwargs)\n","        self.embedding_type = nn.Embedding(num_embeddings=4, embedding_dim=hidden_size, padding_idx=0, **factory_kwargs)\n","        self.pos_embedding = nn.Embedding(num_embeddings=max_length, embedding_dim=hidden_size, **factory_kwargs)\n","        self.device = device\n","        encoderlayer = nn.TransformerEncoderLayer(\n","            d_model=hidden_size,\n","            nhead=num_heads,\n","            dim_feedforward=dim_feedforward_encoder,\n","            dropout=dropout,\n","            batch_first=True,\n","            **factory_kwargs\n","            )\n","        encoder_norm = LayerNorm(hidden_size, **factory_kwargs)\n","        self.encoder = nn.TransformerEncoder(encoder_layer=encoderlayer, num_layers=num_layers, norm=encoder_norm)\n","\n","        self.fc_type = nn.Linear(hidden_size, 3, **factory_kwargs)\n","\n","\n","    def forward(self, src_aid: Tensor, src_type: Tensor,\n","                src_mask: Tensor,\n","                src_key_padding_mask: Optional[Tensor] = None):\n","        src_padding_mask = (src_aid == 0)\n","        src_padding_mask = src_padding_mask.to(src_mask.dtype)\n","        src_padding_mask = src_padding_mask.to(device)\n","#         print(f\"src_type: {src_type}\")\n","        src_aid_embedding = self.embedding_aid(src_aid)\n","        src_type_embedding = self.embedding_type(src_type)\n","        src = src_aid_embedding * src_type_embedding\n","\n","        position_ids = torch.arange(0, src_aid.shape[-1])\n","        position_ids = position_ids.unsqueeze(0).to(device)\n","#         print(position_ids.device)\n","        position = self.pos_embedding(position_ids)\n","\n","        src = src + position\n","\n","        aid_type_embedding = self.encoder(src, mask=src_mask, src_key_padding_mask=src_padding_mask)\n","\n","        output_type = self.fc_type(aid_type_embedding)\n","        # Apply the softmax activation function\n","        output_type = F.softmax(output_type, dim=1)\n","\n","        return aid_type_embedding, output_type\n","\n","\n","    def predict(self, src_aid: Tensor, src_type: Tensor,\n","                src_mask: Optional[Tensor] = None,\n","                src_key_padding_mask: Optional[Tensor] = None):\n","        aid_type_embedding, type_probs = self(src_aid, src_type, src_mask=src_mask)\n","\n","        last_event_embeddings = aid_type_embedding[:, aid_type_embedding.size(1) - 1:, :]\n","        last_event_embeddings = last_event_embeddings.reshape(-1, last_event_embeddings.size(2))\n","        aid_scores = torch.matmul(last_event_embeddings, self.embedding_aid.weight.permute(1, 0)).squeeze(1)\n","\n","        type_probs = type_probs[:, type_probs.size(1) - 1:, :]\n","        type_probs = type_probs.reshape(-1, type_probs.size(2))\n","\n","        return aid_scores, type_probs\n","\n","    def train_step(self, src_aid: Tensor, src_type: Tensor,\n","                    negative_samples: Tensor,\n","                    src_mask: Optional[Tensor] = None):\n","\n","        aid_type_embedding, output_type = self(src_aid, src_type, src_mask)\n","\n","        target_type = src_type[:, 1:].reshape(-1)\n","        target_type = torch.nn.functional.one_hot(target_type, num_classes=4)\n","        target_type = target_type[:, 1:]# remove the 0 class\n","\n","        output_type = output_type[:, :-1, :]#during training, remove the predicted last token\n","        output_type = output_type.reshape(-1, output_type.size(2))\n","        aid_type_embedding = aid_type_embedding[:, :-1, :]#during training, remove the attended last token\n","        aid_type_embedding = aid_type_embedding.reshape(-1, aid_type_embedding.size(2))  # (batch size * sequence_length, embedding dimension)\n","\n","        positives = src_aid[:, 1:]\n","        positives_embeddings = self.embedding_aid(positives)# (batch size, sequence_length, hidden_size)\n","        positives_embeddings = positives_embeddings.reshape(-1, positives_embeddings.size(2))  # (batch size * sequence_length)\n","        negative_samples = negative_samples[:, :-1]  # (batch size, sequence_length)\n","        negatives = negative_samples.reshape(-1)  # (batch size * sequence_length)\n","        negatives_embeddings = self.embedding_aid(negatives)# (batch size * sequence_length, embedding dimension)\n","        positives_scores = (positives_embeddings * aid_type_embedding).sum(1)  # (batch size * sequence_length)\n","        negatives_scores = (negatives_embeddings * aid_type_embedding).sum(1)  # (batch size * sequence_length)\n","\n","        is_target = (positives != 0).float().reshape(-1)  # (batch size * sequence_length)\n","        aid_loss = (- torch.log(torch.sigmoid(positives_scores) + 1e-24) * is_target\n","                    - torch.log(1 - torch.sigmoid(negatives_scores) + 1e-24) * is_target)\n","        type_loss = - target_type * torch.log(output_type)\n","        loss = (aid_loss.sum() + type_loss.sum()) / (is_target.sum() * 2)\n","\n","        return {'loss': loss}\n","\n","    def validation_step(self, src_aid: Tensor, src_type: Tensor,\n","                   src_mask: Optional[Tensor] = None):\n","        aid_scores, type_probs = self.predict(src_aid, src_type, src_mask = src_mask)\n","        aid_positive = src_aid[:, src_aid.size(1) - 1:].reshape(-1) #(batch size )\n","\n","        dcg = ndcg_batch(aid_scores, aid_positive)\n","\n","        # Get the maximum number in the second dimension (axis=1)\n","        type_prediction = torch.argmax(type_probs, dim=1)\n","        type_positive = src_type[:, src_type.size(1) - 1:].reshape(-1) #(batch size )\n","        correct = (type_prediction == type_positive).sum()\n","        # accuracy = ( correct / type_positive.size(0)) * 100\n","\n","        return dcg, correct\n","\n","    def test_step(self, src_aid: Tensor, src_type: Tensor,\n","                        negative_samples: Tensor,\n","                        src_mask: Optional[Tensor] = None):\n","        aid_scores, type_probs = self.predict(src_aid, src_type, src_mask)\n","\n","        return None\n"]},{"cell_type":"markdown","metadata":{"id":"CQQXlFGMGWp6"},"source":["#Train and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBooU8FWwyUg"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import time\n","import os\n","import gc\n","\n","# Define the hyperparameters\n","input_size = 364846 + 1\n","output_size = 364846 + 1\n","num_heads = 4\n","hidden_size = 32 * num_heads\n","num_layers = 2\n","dropout = 0.2\n","max_seq_length = 500 + 1\n","batch_size = 128\n","lr = 0.001\n","num_epochs = 5\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","# Create the model and the optimizer\n","model = TimeSeriesTransformer(num_item=input_size,\n","                              output_size=output_size,\n","                              hidden_size=hidden_size,\n","                              num_heads=num_heads,\n","                              num_layers=num_layers,\n","                              dropout=dropout,\n","                              max_length=max_seq_length,\n","                              device=device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","#loading a checkpoint\n","checkpoint = torch.load('checkpoint_0_.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(device)\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","criterion = nn.CrossEntropyLoss()\n","start_time = time.time()\n","total_loss = 0\n","\n","\n","# for root, dirs, files in os.walk('/kaggle/input/train-data/'):\n","#     for file in files:\n","#         trian_filepaths.append(os.path.join(root, file))\n","train_filepaths = []\n","train_filepaths = ['train_seq_5.npy','train_seq_10.npy', 'train_seq_15.npy', 'train_seq_20.npy', 'train_seq_25.npy',\n","                  'train_seq_30.npy', 'train_seq_35.npy', 'train_seq_40.npy', 'train_seq_50.npy', 'train_seq_100.npy', 'train_seq_150.npy']\n","train_dataset = NumpyDataset(train_filepaths, batch_size)\n","\n","for epoch in range(num_epochs):\n","    train_dataset = NumpyDataset(train_filepaths, batch_size)\n","    batches = next(iter(train_dataset))\n","    print(f\"number of train batches:\")\n","    num_batch = 0\n","    for batch in batches:\n","        event_sequences = batch['items']\n","        negative_samples = torch.from_numpy(batch['negatives']).to(device)\n","        if len(event_sequences) == 1:\n","          print(\"break\")\n","          break\n","        aids_input = torch.from_numpy(np.array([event_sequence[0] for event_sequence in event_sequences]))\n","        types_input = torch.from_numpy(np.array([event_sequence[1] for event_sequence in event_sequences]))\n","        aids_input = aids_input.to(device)\n","        types_input = types_input.to(device)\n","\n","        mask = generate_encoder_input_mask(len(aids_input[0]), len(event_sequences), num_heads)\n","        mask = mask.to(device)\n","        Loss = model.train_step(aids_input, types_input, negative_samples, mask)\n","        optimizer.zero_grad()\n","        loss = Loss['loss']\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss\n","        num_batch += 1\n","    del train_dataset\n","    gc.collect()\n","        # Save a checkpoint every `save_interval` batches\n","    checkpoint_path = f'checkpoint_{epoch}.pt'\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }, checkpoint_path)\n","\n","    # for root, dirs, files in os.walk('/kaggle/input/validation-data/'):\n","    #     for file in files:\n","    #         validation_files.append(os.path.join(root, file))\n","    validation_files=['validation_seq_5.npy','validation_seq_10.npy', 'validation_seq_15.npy', 'validation_seq_20.npy', 'validation_seq_25.npy',\n","                  'validation_seq_30.npy', 'validation_seq_35.npy', 'validation_seq_40.npy', 'validation_seq_50.npy', 'validation_seq_100.npy', 'validation_seq_150.npy']\n","    validation_dataset = NumpyDataset(validation_files, batch_size)\n","    batches = next(iter(validation_dataset))\n","    total_correct = 0\n","    total_dcg = 0\n","    total_seq = 0\n","    for batch in batches:\n","        event_sequences = batch['items']\n","        negative_samples = torch.from_numpy(batch['negatives'])\n","\n","        aids_input = torch.from_numpy(np.array([event_sequence[0] for event_sequence in event_sequences]))\n","        types_input = torch.from_numpy(np.array([event_sequence[1] for event_sequence in event_sequences]))\n","        aids_input = aids_input.to(device)\n","        types_input = types_input.to(device)\n","\n","        mask = generate_encoder_input_mask(len(aids_input[0]), len(event_sequences), num_heads)\n","        mask = mask.to(device)\n","        dcg, correct = model.validation_step(aids_input, types_input, mask)\n","        total_dcg += dcg\n","        total_correct += correct\n","        total_seq += len(event_sequences)\n","\n","    ndcg = total_dcg / total_seq\n","    accuracy = total_correct / total_seq\n","    del validation_dataset\n","    gc.collect()\n","    print(f\"ndcg: {ndcg}\")\n","    print(f\"accuracy: {accuracy}\")\n","    time_duration = time.time() - start_time\n","    print(time_duration)\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO6mGI1Y2xO2D2UFR8/b6WM","gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
